package jiBand;
import java.io.FileNotFoundException;
import java.io.IOException;
import org.datavec.api.records.reader.SequenceRecordReader;
import org.datavec.api.records.reader.impl.csv.CSVSequenceRecordReader;
import org.datavec.api.split.NumberedFileInputSplit;
import org.deeplearning4j.datasets.datavec.SequenceRecordReaderDataSetIterator;
import org.deeplearning4j.datasets.datavec.SequenceRecordReaderDataSetIterator.AlignmentMode;
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.layers.LSTM;
import org.deeplearning4j.nn.conf.layers.RnnOutputLayer;
import org.deeplearning4j.nn.conf.layers.recurrent.LastTimeStep;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.listeners.ScoreIterationListener;
import org.json.simple.parser.ParseException;
import org.nd4j.evaluation.classification.Evaluation;
import org.nd4j.evaluation.regression.RegressionEvaluation;
import org.nd4j.linalg.activations.Activation;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.dataset.api.preprocessor.NormalizerStandardize;
import org.nd4j.linalg.learning.config.Adam;
import org.nd4j.linalg.lossfunctions.LossFunctions;
import org.slf4j.LoggerFactory;
import org.deeplearning4j.nn.conf.RNNFormat;
import org.deeplearning4j.nn.conf.inputs.InputType;

@SuppressWarnings("deprecation")
public class TestClassDL 
{		
	private static RNNFormat rnnDataFormat;

    public TestClassDL(RNNFormat rnnDataFormat){
        this.rnnDataFormat = rnnDataFormat;
    }
 //   @Parameterized.Parameters
    public static Object[] params(){
        return RNNFormat.values();
    }
	private static final org.slf4j.Logger LOGGER = LoggerFactory.getLogger(TestClassDL.class);
	public static void main(String[] args) throws IOException, InterruptedException, FileNotFoundException, ParseException{

		JuxtopiaDataConfig j = new JuxtopiaDataConfig();				//declaring and instantiating the class juxtopia data config as variable j to call files from JSON.
		String trnfeature = j.getJIBandDataConfig("TrainFeatures");		//declaring and initializing trnfeature as a String type to call the  key value pair that contains the absolute path of the file needed for use.
		String tstfeature = j.getJIBandDataConfig("TestFeatures");		//declaring and initializing tstfeature as a String type to call the  key value pair that contains the absolute path of the file needed for use.
		String trnlabel = j.getJIBandDataConfig("TrainLabels");			//declaring and initializing trnlabel as a String type to call the  key value pair that contains the absolute path of the file needed for use.
		String tstlabel = j.getJIBandDataConfig("TestLabels");			//declaring and initializing trnlabel as a String type to call the  key value pair that contains the absolute path of the file needed for use.
		
		int seed = 12345;							//declaring and initializing the seed variable. Seed helps to produce predictable and repeatable data. It is arbitrary number chosen to be used for random number generation.
		double learningRate = 1e-6;				//declaring and initializing the learning rate.
		double exponentialDecay1 = 0.9;			//declaring and initializing the exponential decay I
		double exponentialDecay2 = 0.999;		//declaring and initializing the exponential decay II
		double epsilon = 10e-8;					//declaring and initializing the epsilon
		int miniBatchSize1 = 9;					//declaring and initializing the mini batch size I
		//6540
		int miniBatchSize2 = 9;					//declaring and initializing the mini batch size II
		//10218
		int nEpochs = 20;						//declaring and initializing the Epochs
		int numInputs = 3;						//declaring and initializing the Number of inputs
		int numOutputs = 2;						//declaring and initializing the number of outputs
		int numHiddenNodes = 2;					//declaring and initializing the number of hidden nodes
		int numLabels = 2;						//declaring and initializing the number of possible labels
	
		//Loading the training data
		DataSet trainData;																//declaring the dataset variable for training data
		SequenceRecordReader trainFEATURES = new CSVSequenceRecordReader(0, " ");		//declaring and initializing the Record reader for the training data. These are intended to read sequences of data in CSV format. One file represents one sequence and each line in a file represents one time-step. parameters are number of lines to skip and the delimiter used.
		trainFEATURES.initialize(new NumberedFileInputSplit(trnfeature, 1, 13));		//initializing the feature data coming from the record reader and specifying how many sequences to account for.
		SequenceRecordReader trainLabels = new CSVSequenceRecordReader();				//declaring and initializing the record reader used for the label data.
		trainLabels.initialize(new NumberedFileInputSplit(trnlabel, 1, 13));			//initializing the label data coming from the record reader and specifying how many label sequences to account for.
		SequenceRecordReaderDataSetIterator trainIter =  new SequenceRecordReaderDataSetIterator(trainFEATURES,trainLabels,miniBatchSize1,  numLabels, false, AlignmentMode.ALIGN_END ); //declaring and initializing the dataset iterator with the full parameters including features, labels, mini btach size, possible number of labels, boolean for the type of problem this is  (regression or classification) and an alignment mode.
		//trainIter.setPreProcessor(new LabelLastTimeStepPreProcessor());
		trainData = trainIter.next();				//initializing the training dataset to return the next element in the iteration.
		int nOut =  trainIter.totalOutcomes();
		
		
		//Loading the test data
		DataSet testData;																//declaring the dataset variable for testing data
		SequenceRecordReader testFEATURES = new CSVSequenceRecordReader(0, " ");		//declaring and initializing the Record reader for the testing data. Parameters are number of lines to skip and the delimiter used.
        testFEATURES.initialize(new NumberedFileInputSplit(tstfeature, 1, 11));			//initializing the feature data coming from the record reader and specifying how many sequences to account for.
        SequenceRecordReader testLabels = new CSVSequenceRecordReader();				//declaring and initializing the record reader used for the label data.
		testLabels.initialize(new NumberedFileInputSplit(tstlabel, 1, 11));				//initializing the label data coming from the record reader and specifying how many label sequences to account for.
        SequenceRecordReaderDataSetIterator testIter = new SequenceRecordReaderDataSetIterator(testFEATURES,testLabels,miniBatchSize2, numLabels, false, AlignmentMode.ALIGN_END); //declaring and initializing the dataset iterator with the full parameters including features, labels, mini btach size, possible number of labels, boolean for the type of problem this is  (regression or classification) and an alignment mode.
       // testIter.setPreProcessor(new LabelLastTimeStepPreProcessor());
        testData = testIter.next();	//initializing the testing dataset to return the next element in the iteration.
        
        
        //normalizing the data
        NormalizerStandardize normalizer = new NormalizerStandardize(); //using the normalize standardize normalizer because we are not using image data and the range within the dimensions is not uniform.
        normalizer.fitLabel(true);										// method used to normalize the label and output data within a range
        normalizer.fit(trainData); 										// method fits data for the training data.
        normalizer.transform(trainData);								// method to transform the training data.
       // normalizer.fit(testData);
        //normalizer.transform(testData);
		
		System.out.println("Build model....");
		MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
				//random number generator that helps the program perform better when it comes to mapping the data.
				.seed(seed)
				//adam is a type of optimizer that combines the nest aspects of other gradient models. it is an adaptive rate learning method so it adapts based on the weights and the different parameters. 
					.updater(new Adam(learningRate, exponentialDecay1, exponentialDecay2, epsilon))
					.optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
					.updater(org.deeplearning4j.nn.conf.Updater.NESTEROVS)
					//a regularization method that adds information to solve ill-posed problems as well as preventing overfitting.
					.l2(1e-4)
					.list()					// creates a list builder which allows for the NN layers to be implemented into the multilayer configuration
					.layer(0,new LSTM.Builder().nIn(trainIter.inputColumns()).nOut(numInputs).activation(Activation.TANH).dataFormat(rnnDataFormat).weightInit(WeightInit.RELU).build())			//LSTM layer with parameters including the inputs, outputs, activation function, data format, and weight
					.layer(1,new LSTM.Builder().nIn(numInputs).nOut(numHiddenNodes).activation(Activation.TANH).weightInit(WeightInit.RELU).build())
					.layer(2,new LSTM.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes).activation(Activation.TANH).weightInit(WeightInit.RELU).build())
					.layer(3,new RnnOutputLayer.Builder(LossFunctions.LossFunction.MCXENT).nIn(numHiddenNodes).nOut(nOut).activation(Activation.SOFTMAX).dataFormat(rnnDataFormat).weightInit(WeightInit.XAVIER).build()).setInputType(InputType.recurrent(3, rnnDataFormat)).build(); //LSTM needs and Rnn output layer
					// .backpropType(BackpropType.TruncatedBPTT).tBPTTLength(100)
				
		//creates multi-layer model object.
		MultiLayerNetwork model = new MultiLayerNetwork(conf);
		// initialize multi-layer network.
		model.init();
		model.setListeners(new ScoreIterationListener(20));// sets the frequency of how to print data information
		
		System.out.println("Total number of parameters: " + model.numParams());
		for( int i=0; i<model.getnLayers(); i++ ){
		System.out.println("Layer " + i + " number of parameters: " + model.getLayer(i).numParams());
		}
	
		//initiazes training of the model with training data.
		System.out.println("Train model....");		
	        for (int i = 0; i < nEpochs; i++) 
	        {
	            model.fit(trainData); //fit method works to train the model on all the data all at once.
	            LOGGER.info("Epoch " + i + " complete. Time series evaluation:");
	            
	            Evaluation eval = new Evaluation(2);
	            RegressionEvaluation evaluation = new RegressionEvaluation();
	            INDArray features = testData.getFeatures();			// returns the feature array for the  test data
	            INDArray labels = testData.getLabels();				// returns the label array for the test data
	            INDArray predicted = model.output(features, false);	// returns the output based on all testing data
	            
	            //applies the model to the testing data and evaluates the results.
	            evaluation.evalTimeSeries(labels, predicted);
	            eval.evalTimeSeries(labels, predicted);
	         
	        	//prints the statistics of the model in terms of accuracy and how precise it is.
	            System.out.println("Evaluate model....");
	           // System.out.println(evaluation.stats());
	            System.out.println(eval.stats());
	        }
	        model.rnnTimeStep(trainData.getFeatures());
	        INDArray predicted = model.rnnTimeStep(testData.getFeatures());

	        //Revert data back to original values for plotting
	        normalizer.revert(trainData);
	        normalizer.revert(testData);
	        normalizer.revertLabels(predicted);

	 //      INDArray trainFeatures = trainData.getFeatures();
	 //      INDArray testFeatures = testData.getFeatures();

	        LOGGER.info("----- Example Complete -----");
	}
}
