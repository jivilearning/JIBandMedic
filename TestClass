package jiBand;
import java.io.FileNotFoundException;
import java.io.IOException;
import org.datavec.api.records.reader.SequenceRecordReader;
import org.datavec.api.records.reader.impl.csv.CSVSequenceRecordReader;
import org.datavec.api.split.NumberedFileInputSplit;
import org.deeplearning4j.datasets.datavec.SequenceRecordReaderDataSetIterator;
import org.deeplearning4j.datasets.datavec.SequenceRecordReaderDataSetIterator.AlignmentMode;
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.layers.LSTM;
import org.deeplearning4j.nn.conf.layers.RnnOutputLayer;
import org.deeplearning4j.nn.conf.layers.recurrent.LastTimeStep;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.listeners.ScoreIterationListener;
import org.json.simple.parser.ParseException;
import org.nd4j.evaluation.classification.Evaluation;
import org.nd4j.evaluation.regression.RegressionEvaluation;
import org.nd4j.linalg.activations.Activation;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.dataset.api.preprocessor.NormalizerStandardize;
import org.nd4j.linalg.learning.config.Adam;
import org.nd4j.linalg.lossfunctions.LossFunctions;
import org.slf4j.LoggerFactory;
import org.deeplearning4j.nn.conf.RNNFormat;
import org.deeplearning4j.nn.conf.inputs.InputType;

@SuppressWarnings("deprecation")
public class TestClassDL 
{		
	private static RNNFormat rnnDataFormat;

    public TestClassDL(RNNFormat rnnDataFormat){
        this.rnnDataFormat = rnnDataFormat;
    }
 //   @Parameterized.Parameters
    public static Object[] params(){
        return RNNFormat.values();
    }
	private static final org.slf4j.Logger LOGGER = LoggerFactory.getLogger(TestClassDL.class);
	public static void main(String[] args) throws IOException, InterruptedException, FileNotFoundException, ParseException{

		JuxtopiaDataConfig j = new JuxtopiaDataConfig();								//declaring and instantiating the class juxtopia data config as variable j.
		String trnfeature = j.getJIBandDataConfig("TrainFeatures");
		String tstfeature = j.getJIBandDataConfig("TestFeatures");
		String trnlabel = j.getJIBandDataConfig("TrainLabels");
		String tstlabel = j.getJIBandDataConfig("TestLabels");
		//instantiating and initializing the base directory to look from
		int seed = 123;
		double learningRate = 1e-6;
		double exponentialDecay1 = 0.9;
		double exponentialDecay2 = 0.999;
		double epsilon = 10e-8;
		int miniBatchSize1 = 2;
		//6540
		int miniBatchSize2 = 3;
		//10218
		int nEpochs = 10;
		int numInputs = 3;
		int numOutputs = 2;
		int numHiddenNodes = 2;
		int labelIndex = 3;
		int numLabels = 2;
	
		//Loading the training data
		DataSet trainData;
		SequenceRecordReader trainFEATURES = new CSVSequenceRecordReader(0, " ");
		trainFEATURES.initialize(new NumberedFileInputSplit(trnfeature, 1, 13));
		SequenceRecordReader trainLabels = new CSVSequenceRecordReader();
		trainLabels.initialize(new NumberedFileInputSplit(trnlabel, 1, 13));
		SequenceRecordReaderDataSetIterator trainIter =  new SequenceRecordReaderDataSetIterator(trainFEATURES,trainLabels,miniBatchSize1,  numLabels, false, AlignmentMode.ALIGN_END );
		//trainIter.setPreProcessor(new LabelLastTimeStepPreProcessor());
		trainData = trainIter.next();

		//Loading the test data
		DataSet testData;
		SequenceRecordReader testFEATURES = new CSVSequenceRecordReader(0, " ");
        testFEATURES.initialize(new NumberedFileInputSplit(tstfeature, 1, 11));
        SequenceRecordReader testLabels = new CSVSequenceRecordReader();
		testLabels.initialize(new NumberedFileInputSplit(tstlabel, 1, 11));
        SequenceRecordReaderDataSetIterator testIter = new SequenceRecordReaderDataSetIterator(testFEATURES,testLabels,miniBatchSize2, numLabels, false, AlignmentMode.ALIGN_END);
       // testIter.setPreProcessor(new LabelLastTimeStepPreProcessor());
        testData = testIter.next();
        
        //normalizing the data
        NormalizerStandardize normalizer = new NormalizerStandardize(); //using the normalize standardize normalizer because we are not using image data and the range within the dimensions is not uniform.
        normalizer.fitLabel(true);
        normalizer.fit(trainData); 
        normalizer.transform(trainData);
       // normalizer.fit(testData);
        //normalizer.transform(testData);
		
		System.out.println("Build model....");
		MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
				//random number generator that helps the program perform better when it comes to mapping the data.
				.seed(seed)
				//adam is a type of optimizer that combines the nest aspects of other gradient models. it is an adaptive rate learning method so it adapts based on the weights and the different parameters. 
					.updater(new Adam(learningRate, exponentialDecay1, exponentialDecay2, epsilon))
					.optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
					.updater(org.deeplearning4j.nn.conf.Updater.NESTEROVS)
					//a regularization method that adds information to solve ill-posed problems as well as preventing overfitting.
					.l2(1e-4)
					.list()
					.layer(0, new LastTimeStep (new LSTM.Builder().nIn(numInputs).nOut(numHiddenNodes).activation(Activation.TANH).dataFormat(rnnDataFormat).weightInit(WeightInit.RELU).build()))
					//.layer(1, new LastTimeStep(new LSTM.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes).activation(Activation.TANH).weightInit(WeightInit.RELU).build()))
					.layer(1, new RnnOutputLayer.Builder(LossFunctions.LossFunction.MCXENT).nIn(numHiddenNodes).nOut(numOutputs).activation(Activation.SOFTMAX).dataFormat(rnnDataFormat).weightInit(WeightInit.XAVIER).build()).setInputType(InputType.recurrent(3, rnnDataFormat)).build();
					// .backpropType(BackpropType.TruncatedBPTT).tBPTTLength(100)
				
		//creates multi-layer model object.
		MultiLayerNetwork model = new MultiLayerNetwork(conf);
		// initialize multi-layer network.
		model.init();
		model.setListeners(new ScoreIterationListener(20));
	
		//initiazes training of the model with training data.
		System.out.println("Train model....");		
	        for (int i = 0; i < nEpochs; i++) 
	        {
	            model.fit(trainData);
	            LOGGER.info("Epoch " + i + " complete. Time series evaluation:");
	            
	            Evaluation eval = new Evaluation();
	            RegressionEvaluation evaluation = new RegressionEvaluation();
	            INDArray features = testData.getFeatures();
	            INDArray labels = testData.getLabels();
	            INDArray predicted = model.output(features, true);
	            
	            //applies the model to the testing data and evaluates the results.
	            evaluation.evalTimeSeries(labels, predicted);
	            eval.evalTimeSeries(labels, predicted);
	         
	        	//prints the statistics of the model in terms of accuracy and how precise it is.
	            System.out.println("Evaluate model....");
	            System.out.println(evaluation.stats());
	            System.out.println(eval.stats());
	        }
	        model.rnnTimeStep(trainData.getFeatures());
	        INDArray predicted = model.rnnTimeStep(testData.getFeatures());

	        //Revert data back to original values for plotting
	        normalizer.revert(trainData);
	        normalizer.revert(testData);
	        normalizer.revertLabels(predicted);

	 //      INDArray trainFeatures = trainData.getFeatures();
	 //      INDArray testFeatures = testData.getFeatures();

	        LOGGER.info("----- Example Complete -----");
	}
}
