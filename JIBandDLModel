import java.io.File;
import java.util.Scanner;

import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration.Builder;
import org.deeplearning4j.nn.conf.layers.OutputLayer;
import org.deeplearning4j.nn.layers.recurrent.LSTM;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.nd4j.evaluation.classification.Evaluation;
import org.nd4j.linalg.activations.Activation;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.api.ops.impl.layers.recurrent.LSTMLayer;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.learning.config.Adam;
import org.nd4j.linalg.lossfunctions.LossFunctions.LossFunction;



public class JIBandDLModel {

	private static final String IMU_DATASET_ROOT_FOLDER = "/Users/NiiArmah/JET_Training/AMS_Dataset/";
	private static final int COLUMNS = 3;
	private static final int ROWS = 100;
	private static final int N_SAMPLES_TRAINING = 4;
	private static final int N_SAMPLES_TESTING = 4;
	private static final int N_OUTCOMES= 2;
	private static final long t0 = System.currentTimeMillis();
	
	private static DataSetIterator getDataSetIterator(String folderPath, int nSamples)  {
		
		File folder = new File(folderPath);
		return null;
		Scanner sc = null;
		
	}
	
	public static void main(String[] args) {
		
		DataSetIterator dsi = getDataSetIterator(IMU_DATASET_ROOT_FOLDER + "training", N_SAMPLES_TRAINING);
		
		int rngSeed = 123; 
		int nEpochs = 2; 
		
		System.out.println("Build model....");
		MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
				//random number generator that helps the program perform better when it comes to mapping the data.
				.seed(rngSeed)
				//adam is a type of optimizer that combines the nest aspects of other gradient models. it is an adaptive rate learning method so it adapts based on the weights and the different parameters. 
				.updater(new Adam(0.006,0.9, 1.0, 0.00000000009))
				//a regularization method that adds information to solve ill-posed problems as well as preventing overfitting.
				.l2(1e-4)
				.list()
				.layer(new GravesLSTM.Builder()
					.nIn(COLUMNS*ROWS)
					.nOut(100)
					.activation(Activation.RELU)
					.weightInit(WeightInit.XAVIER)
					.build())
				.layer(new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD))
					.nIn(100)
					.nOut(N_OUTCOMES)
					.activation(Activation.SOFTMAX)
					.weightInit(WeightInit.XAVIER)
					.build()
				.build();
		
		MultiLayerNetwork model = new MultiLayerNetwork(conf);
		model.init();
		System.out.println("Train model....");
		
		DataSetIterator testDsi = getDataSetIterator(IMU_DATASET_ROOT_FOLDER + "testing", N_SAMPLES_TESTING);
		System.out.println("Evaluate model....");
		Evaluation eval = model.evaluate(testDsi);
		System.out.println(eval.stats()); 
		
		long t1 = System.currentTimeMillis();
		double t = (double)(t1 - t0) / 1000.0;
		System.out.println("\n\nTotal time: "+t+" seconds");
	
	}

}
